from typing import Union
import os
import glob
import wandb
import torch
import numpy as np
from pytorch_lightning.callbacks import Callback
import pytorch_lightning as pl


class ArtifactLogger(Callback):
    LOCAL_MODEL_DIR = 'checkpoints/'

    def __init__(self) -> None:
        '''
        A pytorch lightning Callback for logging model artifacts to wandb after training, used together with ModelCheckpoint.
        Currently using hydra cwd so that all models under ./checkpoints are generated by the current run. No need to apply modified time filters. Log all models to wandb.

        '''
        super().__init__()

    def on_fit_end(self, trainer: "pl.Trainer", pl_module: "pl.LightningModule") -> None:
        checkpoints = glob.glob(self.LOCAL_MODEL_DIR + '*.ckpt')
        for ckpt in checkpoints:
            name = os.path.basename(ckpt)
            aliases = [f'{pl_module.dataset.name}'] #TODO: add arch alias
            model_artifact = wandb.Artifact(name, 
                                    type='model', 
                                    # aliases=aliases,
                                    # description=description
                                    )
            model_artifact.add_file(ckpt)
            wandb.run.log_artifact(model_artifact)